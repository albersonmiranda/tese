---
format:
    pdf:
        include-in-header: config/tema/preamble_pre-projeto.tex
        cite-method: biblatex
        output-file: pre-projeto.pdf
        documentclass: abntex2
        classoption: [12pt, oneside, a4paper, chapter=TITLE, section=TITLE, brazil]
        latex-max-runs: 4
---

```{r setup, include=FALSE}

# pacotes
library(kableExtra)
```

\noindent TÍTULO DO PROJETO: Reconciliação Ótima de Séries Temporais Hierárquicas Através de Regressão Regularizada

\noindent LINHA DE PESQUISA: Métodos e Modelos Matemáticos, Econométricos e Estatísticos Aplicados à Economia

# INTRODUÇÃO

No que concerne a elaboração de *forecasting* no mercado, diversas indústrias requerem previsões de múltiplas séries temporais correlacionadas que são resultados de agregação. Por exemplo, o total de empréstimos de uma instituição financeira corresponde ao agregado dos empréstimos de cada uma de suas agências; o total de vendas de uma rede nacional de farmácias corresponde ao agregado de vendas de suas unidades em cada estado; o total da produção de uma petrolífica multinacional corresponde ao total produzido em cada país por cada uma de suas plataformas. A essas estruturas naturais de agregação dá-se o nome de *séries temporais hierárquicas*.

Séries temporais hierárquicas são aquelas que podem ser agregadas ou desagregadas naturalmente em uma estrutura aninhada [@hyndman_forecasting_2021]. Para ilustrar, tome a série do PIB de um país fictício com três estados, cada um com dois municípios. Essa série pode ser desagregada por estado que, por sua vez, pode ser desagregada por município (@fig-h).

![Séries Hierárquicas](img/hierarq.png){#fig-h}

Pode-se realizar previsões individualmente para todos todos os níveis da estrutura. No caso do PIB nacional, isso significa realizar previsões, por exemplo, para cada município, para cada estado e para o agregado do país. Infelizmente, não há qualquer razão, exceto para métodos de previsão muito simples, para que essas previsões sejam *coerentes* (i.e. que a soma das previsões individuais seja igual à previsão do agregado). Além disso, realizar as previsões individualmente ignoraria os relacionamentos existentes entre as séries temporais na estrutura. Para fazer com que essas previsões se tornem coerentes entre si é que foram desenvolvidos os chamados métodos de *reconciliação*, sendo os mais simples o *top-down*, *bottom-up* e uma combinação das duas, a *middle-out*.

A prática usual em *budgeting*^[O orçamento é um documento no qual é definido o planejamento financeiro de um empresa, geralmente para o ano seguinte, estabelecendo metas e objetivos. Nele são projetadas as expectativas da empresa e é base de comparação para saber como os resultados estão se desviando da performance esperada.], principalmente para empresas com muitas filiais, é a *top-down*, ou seja, realizar previsões para o total agregado e então distribuí-las para cada unidade seguindo alguma lógica proporcional. No entanto, conforme descemos na hierarquia, menos precisa ela se torna e, além disso, as características individuais das séries temporais do menor nível hierárquicos são ignoradas.

Tomando o caminho inverso, a abordagem *bottom-up* consiste em realizar previsões para cada série temporal individualmente e, então, agregá-las para obter a previsão para o total. Essa abordagem pode ser mais precisa, pois leva em consideração as características individuais de cada série temporal do nível mais desagregado. No entanto, ela é mais custosa em termos de tempo de processamento e análise. Nesse sentido, cabe ao analista avaliar o *trade-off* entre os ganhos de precisão percebidos com a geração de previsões individuais e a economia de tempo e processamento em realizar o contrário [@gross_disaggregation_1990].

Ambas são abordagens de nível único, isto é, são realizadas as previsões para um único nível e então os demais níveis são obtidos agregando ou desagregando. O problema com esses tipos de abordagem é que elas utilizam informação incompleta [@hyndman_forecasting_2021]. No caso do *bottom-up*, ignora-se a influência que os níveis mais agregados pode ter na estimação de cada elemento individual (por exemplo, uma filial qualquer). Por outro lado, se escolher estimar modelos para os níveis mais agregados (*top-down*), ignora-se a informação individual de cada elemento.

Uma terceira possibilidade é a *reconciliação ótima*. Ela é uma abordagem que busca resolver esse problema e consiste em realizar previsões para todos os níveis hierárquicos e, então, estimar um modelo para reescrever as previsões do nível mais desagregado como uma combinação linear de todos os elementos da hierarquia. Obtidas as novas previsões no menor nível, ela são então agregadas, gerando previsões coerentes nos níveis superiores. Dessa forma, a informação de todos os níveis é utilizada na estimação dos modelos e na geração das previsões, ao mesmo tempo em que a variância do erro de previsão é minimizado [@hyndman_optimal_2011].

Atualmente, os métodos analíticos baseados na minimização do traço da matriz da variância-covariância dos erros, desenvolvidos em  @wickramasuriya_optimal_2019, são os mais populares na literatura da reconciliação ótima. Esses métodos divergem apenas na forma da qual se dá o relacionamento entre os diferentes elementos da hierarquia: se os erros de previsão são descorrelacionados e equivariantes ao longo de toda estrutura (MQO), o que é impossível em séries temporais hierárquicas; se os erros são descorrelacionados e equivariantes apenas dentro do mesmo nível hierárquico (MQP estrutural); se os erros são descorrelacionados mas ponderados pela variância da série (MQP); ou se são correlacionados e variantes ao longo de toda a estrutura (estimadores *MinT Sample* e *MinT Shrink*). Entretanto, tais métodos são sujeitos a uma série de restrições, como as do modelo clássico de regressão linear, e têm sua capacidade preditiva reduzida quando suas hipóteses são violadas.

Em previsões de séries temporais, o objetivo na maioria dos casos é prever valores futuros com a maior acurácia possível. Em vista disso, métodos de *machine learning* são mais gerais, no sentido de permitir parâmetros não lineares e poderem aproximar virtualmente qualquer função. Além disso, são focados na capacidade preditiva, muitas vezes em detrimento da explicativa. Espera-se, portanto, que esses métodos alcancem melhor performance no problema da reconciliação ótima, justificando a pesquisa e atenção ao tema. Nesse sentido, trabalhos como @spiliotis_hierarchical_2021 desenvolvem métodos de reconciliação ótima de séries temporais utilizando algoritmos de *machine learning*, especificamente o *XGBoost* e *Random Forest*, obtendo resultados superiores aos métodos analíticos tradicionais, especialmente quando as séries não possuem características semelhantes.

Tomando como ponto de partida as conclusões de @spiliotis_hierarchical_2021, este trabalho busca continuar e estender a exploração de métodos de *machine learning* para a tarefa de reconciliação ótima, especificamente o método de regressão regularizada *elastic net*, verificando se sua performance se mantêm superior à dos métodos analíticos em um contexto de séries temporais de alta dimensionalidade.

# OBJETIVOS

O objetivo geral do trabalho é estudar o problema da reconciliação ótima de previsões pontuais a partir de métodos de regressão regularizada.

Os objetivos específicos são:

1. Atestar a performance do método *elastic net* para reconciliação ótima de previsões pontuais de séries temporais hierárquicas e agrupadas, verificando sua validade em dados de alta dimensionalidade, através de uma variedade de experimentos de Monte Carlo e um exemplo empírico com dados de séries temporais de saldos de crédito das instituições financeiras brasileiras;
1. Comparar a performance, em termos de funções de custo e tempo de processamento, do método *elastic net* nas configurações $\alpha=0$ (*lasso*), $\alpha=1$ (*ridge*) e uma combinação de ambos obtida via reamostragem;
1. Comparar a performance, em termos de funções de custo e tempo de processamento, do método *elastic net* com os métodos analíticos *bottom-up*, *top-down* e *MinT Shrink*;
1. Comparar a performance, em termos de funções de custo e tempo de processamento, das estratégias de reamostragem validação cruzada $k$*-fold* e *holdout* para o método *elastic net*;
1. Verificar se a aplicação de regressão regularizada resulta em algum padrão reconhecível na estrutura (e.g., se a regularização tende a favorecer os coeficientes de séries temporais que compartilham o mesmo nó pai ou, em outras palavras, se tende a zerar os coeficientes de séries temporais que não compartilham o mesmo nó pai na hierarquia);

# FUNDAMENTAÇÃO TEÓRICA

Previsões pontuais de séries temporais hierárquicas não é um assunto novo. Ao menos desde a década de 70, pesquisas foram publicadas acerca de abordagens *bottom-up* e *top-down*, suas vantagens e desvantagens, e tentativas de se definir qual é o melhor método^[Uma revisão dessa literatura pode ser encontrada em @athanasopoulos_hierarchical_2009.]. Entretanto, é apenas em @hyndman_optimal_2011 que é formalizada uma abordagem prática que utiliza toda a informação disponível, (i.e. as previsões de todos elementos de todos os níveis da hierarquia) a partir da estimação de uma matriz de pesos via regressão linear por mínimos quadrados generalizados (MQG).

Entretanto, para ser capaz de estimar o modelo por MQG, é necessária a matriz de variância-covariância dos erros. @hyndman_optimal_2011 usam a matriz de erros de coerência, ou seja, a diferença entre as previsões reconciliadas e as previsões individuais, que tem posto incompleto e não identificada e, portanto, não pode ser estimada. Os autores contornam esse problema adotando no lugar da matriz de variância-covariância dos erros uma matriz diagonal constante, ou seja, assumem variância constante dos erros de reconciliação, e estimam a matriz de pesos por mínimos quadrados ordinários (MQO).

A estimação por esse método resulta numa reconciliação ótima que depende apenas da estrutura hierárquica e independe da variância e covariância das previsões individuais --- o que não é uma conclusão satisfatória, uma vez que é impossível que uma série hierárquica seja descorrelatada, pois um nível superior é, necessariamente, o agregado do inferior.

@hyndman_fast_2016 tentam aperfeiçoar o método usando as variâncias das previsões individuais estimadas (dentro da amostra) como estimativa para a matriz de variância-covariância dos erros de reconciliação, de forma a as utilizar como pesos e realizar a reconciliação ótima por mínimos quadrados ponderados (MQP). Assim, previsões individuais mais acuradas têm peso maior do que as mais ruidosas. Entretanto, não fornecem justificativa teórica para usar a diagonal da matriz de variância-covariância de $\mathbfit{\hat{e}_{t}}$.

@wickramasuriya_optimal_2019 argumentam que o que de fato interessa é que as previsões reconciliadas tenham o menor erro. Então, corrigem a abordagem de reconciliação ótima para o objetivo de minimização dos erros das previsões reconciliadas $\mathbfit{\tilde{y}_{t+h}}$, ao invés dos erros das previsões individuais $\mathbfit{\hat{y}_{t+h}}$. Dado que isso implica na minimização da variância de $\mathbfit{\tilde{e}_{t+h}}$, ou seja, na minimização do somatório da diagonal, o traço, da matriz de variância-covariância de $\mathbfit{\tilde{e}_{t+h}}$, eles chamaram esse método de Traço Mínimo (*MinT*, na sigla em inglês). Paralelamente, usam desigualdade triangular para demonstrar que as previsões reconciliadas obtidas por esse método são ao menos tão boas quanto as previsões individuais.

@panagiotelis_forecast_2021 reinterpreta a literatura de coerência e reconciliação de previsões pontuais a partir de uma abordagem geométrica, trazendo provas alternativas para conclusões anteriores ao mesmo tempo em que fornece novos teoremas. Além disso, os autores estendem essa interpretação geométrica para o contexto probabilístico, fornecendo métodos paramétricos e não paramétricos (via *bootstrapping*) para reconciliação de previsões probabilísticas, ou seja, para reconciliar previsões $\hat{y}_t$ obtidas a partir de toda a distribuição, e não apenas a média.

@spiliotis_hierarchical_2021 propõem a utilização de *machine learning* para a reconciliação ótima de séries temporais, especificamente os algoritmos de árvore de decisão *Random Forest* e *XGBoost*. Os autores descrevem como vantagens desse método em relação aos anteriores a descrição de relacionamentos não lineares, performance preditiva e a desnecessidade da utilização de todos os elementos da hierarquia na combinação ótima. Para o conjunto de dados utilizados, os autores afirmam que os métodos de *machine learning*, especialmente o XGBoost, alcançaram, em média, melhor performance que as abordagens de nível único e o *MinT*. Além disso, concluíram que quanto maior é a diferença entre as séries, em todos os níveis hierárquicos, maior são os benefícios da abordagem por *machine learning*.

## O método elastic net

O *elastic net* [@zou_regularization_2005] é um método de regressão regularizada que combina as normas $L_1$ e $L_2$, as penalidades do *lasso* e do *ridge*, respectivamente. A função objetivo a ser minimizada é dada por

$$
L(\lambda_1, \lambda_2, \mathbfit{\beta}) = |\mathbf{y} - \mathbf{X}\mathbfit{\beta}|^2 + \lambda_2|\mathbfit{\beta}|^2 + \lambda_1|\mathbfit{\beta}|_1
$$ {#eq-elastic_net}

\noindent em que $\lambda_1$ e $\lambda_2$ são os parâmetros de regularização e $\mathbfit{\beta}$ é o vetor de coeficientes a serem estimados. A solução para essa função objetivo é dada por^[Sob o valor otimizado ainda é aplicada correção de escala na forma $(1+\lambda_2)\mathbfit{\hat{\beta}}$. Ver @zou_regularization_2005.]

$$
\mathbfit{\hat{\beta}} = \arg \min_{\mathbfit{\beta}} |\mathbf{y}-\mathbf{X}\mathbfit{\beta}|^2 \text{, sujeito a } (1-\alpha)|\mathbfit{\beta}|_1 + \alpha|\mathbfit{\beta}|^2 \leq t
$$ {#eq-elastic_net_solution}

\noindent com $\alpha = \frac{\lambda_2}{\lambda_1 + \lambda_2}$ e $t \in \mathbb{R}^+$.

A função $(1-\alpha)|\mathbfit{\beta}|_1 + \alpha|\mathbfit{\beta}|^2$ é a penalidade *elastic net*, uma combinação das penalidades *lasso* e *ridge*. O parâmetro $\alpha$ controla a mistura das duas penalidades, incluindo os casos extremos. Note que $\alpha = 0 \implies \lambda_2 = 0$, resultando em uma penalidade exclusivamente *lasso*, enquanto $\alpha = 1 \implies \lambda_1 = 0$, e a penalidade é apenas do tipo *ridge*.

Portanto o *elastic net* é um método de *shrinkage*, uma vez que a penalidade *ridge* reduz o tamanho dos coeficientes, e de *seleção de variáveis*, uma vez que a penalidade *lasso* tende a anular os coeficientes de variáveis irrelevantes. Essas propriedades são desejáveis para a reconciliação de séries temporais, uma vez que a estrutura hierárquica pode conter séries insignificantes para a previsão de outras séries.

Diferentemente dos métodos analíticos estudados, o *elastic net* não possui uma solução fechada. Portanto, é necessário utilizar métodos iterativos para encontrar o valor ótimo de $\mathbfit{\hat{\beta}}$ e [@zou_regularization_2005] utilizam validação cruzada $k$-fold para encontrar quais os valores de $\lambda_1$ e $\lambda_2$ que minimizam o resíduo. Nesse sentido, dado a metodologia de processo iterativo envolvendo calibragem de hiperparâmetros e reamostragem, podemos classificar o *elastic net* como um método de *machine learning*.

# PROCEDIMENTOS METODOLÓGICOS

## Processo de geração de dados para simulações de Monte Carlo

Os experimentos de Monte Carlo devem ser projetados para explorar:

1. Os efeitos do ruído de previsão. O objetivo aqui é verificar se o método *elastic net* exibe aumento ou deterioração de performance em relação aos métodos analíticos de reconciliação ótima quando as previsões individuais são mais ou menos ruidosas (i.e. se a variância do erro das previsões individuais é maior ou menor);
1. Os efeitos de correlação entre as séries temporais do menor nível hierárquico. O objetivo aqui é verificar se o método *elastic net* exibe aumento ou deterioração de performance em relação aos métodos analíticos de reconciliação ótima quando as séries temporais do menor nível hierárquico são mais ou menos correlacionadas;
1. Os efeitos de componentes sazonais. O objetivo aqui é verificar se o método *elastic net* exibe aumento ou deterioração de performance em relação aos métodos analíticos de reconciliação ótima quando as séries temporais do menor nível hierárquico possuem ou não componentes sazonais;
1. Os efeitos do tamanho da hierarquia. O objetivo aqui é verificar se o método *elastic net* exibe aumento ou deterioração de performance em relação aos métodos analíticos de reconciliação ótima quando a hierarquia é mais ou menos profunda (i.e. possui mais ou menos níveis hierárquicos).

## Dados e variáveis para experimento empírico

Os dados para esse trabalho podem ser obtidos do *datalake* público Base dos Dados [@dahis_data_2022]. A fonte primária são os bancos comerciais e múltiplos com carteira comercial que disponibilizam mensalmente os saldos dos principais verbetes do balancete via documento 4500^[Esses documentos são relatórios eletrônicos obrigatórios demandados pelo Bacen às instituições financeiras que permitem ao regulador o conhecimento minucioso dos bancos e de seus clientes.] ao Banco Central do Brasil, que os compila e publica, agrupados por agência bancária e por município, no relatório ESTBAN — Estatística Bancária Mensal e por Município^[https://www4.bcb.gov.br/fis/cosif/estban.asp?frame=1].

O que compõe os verbetes de crédito, ou seja, os valores das séries temporais a serem trabalhadas, são os saldos de crédito ativo (empréstimos e financiamentos), que são correspondem ao principal mais os juros calculados até 59 dias de atraso^[Não são consideradas crédito ativo as operações de crédito liquidadas ou que tenham sido transferidas para prejuízo. São transferidas para prejuízo as operações de crédito em atraso há mais 6 meses após sua classificação de risco em H, que é a mínima [@conselho_monetario_nacional_resolucao_1999].].

Além das estatísticas bancárias, serão obtidas informações de regiões, mesorregiões e microrregiões dos estados, também a partir *datalake* Base dos Dados, com o objetivo de enriquecer a estrutura hierárquica dos dados do ESTBAN, limitada aos municípios. Quanto ao período, há dados disponíveis desde 1988. Entretanto, utilizaremos os dados a partir de 2010 pois, se tratando de uma hierarquia larga, o custo computacional deve ser levado em conta. Por fim, as variáveis mantidas no *dataset* foram as descritas no Quadro \ref{qdr-variaveis}.

```{r}
#| tbl-cap: "Variáveis do dataset"
#| label: qdr-variaveis
#| include: false

tibble::tribble(
    ~Variável, ~Descrição,
    "ref", "Data de referência do relatório ESTBAN",
    "nome_regiao", "Nome da região",
    "nome_uf", "Nome do estado",
    "nome_mesorregiao", "Nome da mesorregião da UF",
    "nome", "Nome do município",
    "cnpj_agencia", "CNPJ da agência bancária",
    "verbete", "Descrição da rubrica do balancete",
    "valor", "Saldo do verbete"
) |>
    kbl(booktabs = TRUE, row.names = FALSE, format = "latex") |>
    kable_styling(latex_options = c("striped"), font_size = 12) |>
    column_spec(2, width = "10cm") |>
writeLines()
```

```{=latex}
\begin{quadro}
    \caption{Variáveis do dataset}\tabularnewline
    \label{qdr-variaveis}

    \centering\begingroup\fontsize{12}{14}\selectfont

    \begin{tabular}[t]{l>{\raggedright\arraybackslash}p{10cm}}
    \toprule
    Variável & Descrição\\
    \midrule
    \cellcolor{gray!6}{ref} & \cellcolor{gray!6}{Data de referência do relatório ESTBAN}\\
    nome\_regiao & Nome da região\\
    \cellcolor{gray!6}{nome\_uf} & \cellcolor{gray!6}{Nome do estado}\\
    nome\_mesorregiao & Nome da mesorregião da UF\\
    \cellcolor{gray!6}{nome} & \cellcolor{gray!6}{Nome do município}\\
    \addlinespace
    cnpj\_agencia & CNPJ da agência bancária\\
    \cellcolor{gray!6}{verbete} & \cellcolor{gray!6}{Descrição da rubrica do balancete}\\
    valor & Saldo do verbete\\
    \bottomrule
    \end{tabular}
    \endgroup{}
\end{quadro}
```
## Previsões base

Uma vez que o foco deste trabalho é no ganho proporcionado pela reconciliação ótima, as previsões base serão obtidas por meio do métodos básicos para previsão de séries temporais, como algum tipo de suavimento exponencial ou pelo algoritmo de Hyndman-Khandakar [@hyndman_forecasting_2021]. Esses métodos são amplamente utilizados na literatura de séries temporais e, portanto, servem como *benchmark* para a avaliação dos métodos de reconciliação ótima.

## *Design* dos experimentos

Os métodos analíticos de reconciliação utilizados no *benchmark* serão os de nível único *bottom-up* e o *top-down*; os de reconciliação ótima *MinT-OLS*, *MinT-WLS* e *MinT-shrink*. Essa escolha se justifica para que seja possível comparar os métodos de reconciliação ótima com os métodos de nível único, principalmente em relação ao tempo de processamento, que pode revelar um *trade-off* a ser analisado pelo pesquisador.

Já a metodologia para obtenção das previsões reconciliadas por métodos baseados em *machine-learning* será semelhante ao de @spiliotis_hierarchical_2021:

1. dividir a amostra em treino e teste;
1. treinar um modelo de previsão na amostra treino e obter previsões um passo a frente para a amostra teste;
1. treinar um modelo de *machine learning* para cada série do menor nível da hierarquia, em que os parâmetros são as previsões obtidas no passo 2 e a variável explicada são os valores observados. Isso resulta em um modelo de reconciliação ótima para cada elemento do menor nível da hierarquia, combinando informações disponíveis de todos os níveis hierárquicos;
1. obter as previsões base $\hat{y}_t$;
1. passar as previsões base ao modelo treinado no passo 3 para se obter as previsões reconciliadas para o menor nível da hierarquia;
1. agregar as previsões reconciliadas para se obter as previsões nos demais níveis hierárquicos.

Esse procedimento será realizado três vezes, uma para cada configuração do método *elastic net*, sendo $\alpha=0$ (*lasso*), $\alpha=1$ (*ridge*) e uma combinação de ambos obtida via reamostragem. Espera-se que todos os três modelos propostos nesse trabalho sejam capazes de superar os métodos analíticos de reconciliação.